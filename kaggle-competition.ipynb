{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ECBM E4040 - Assignment 2- Task 5: Kaggle Open-ended Competition\n",
    "\n",
    "Kaggle is a platform for predictive modelling and analytics competitions in which companies and researchers post data and statisticians and data miners compete to produce the best models for predicting and describing the data.\n",
    "\n",
    "If you don't have a Kaggle account, feel free to join at [www.kaggle.com](https://www.kaggle.com). To let the CAs do the grading more conveniently, please __use Lionmail to join Kaggle__ and __use UNI as your username__.\n",
    "\n",
    "The competition is located here: https://www.kaggle.com/t/d908ef03b7244102a1e006516a6555a6\n",
    "\n",
    "You can find detailed description about this in-class competition on the website above. Please read carefully and follow the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO__:</span>\n",
    "\n",
    "- Train a custom model for the bottle dataset classification problem. You are free to use any methods taught in the class or found by yourself on the Internet (ALWAYS provide reference to the source). General training methods include:\n",
    "  - Dropout\n",
    "  - Batch normalization\n",
    "  - Early stopping\n",
    "  - l1-norm & l2-norm penalization\n",
    "- You are given the test set to generate your predictions (70% public + 30% private, but you don't know which ones are public/private). Students should achieve an accuracy on the public test set of at least 70%. Two points will be deducted for each 1% below 70% accuracy threshold (i.e. 65% accuracy will have 10 points deducted). The accuracy will be shown on the public leaderboard once you submit your prediction .csv file. The private leaderboard will be released after the competition. The final ranking is based on the private leaderboard result, not the public leaderboard.\n",
    "\n",
    "\n",
    "NOTE: \n",
    "* Report your results on the Kaggle, for comparison with other students' optimal results (you can do this several times). \n",
    "* Save your best model.\n",
    "\n",
    "__Hint__: You can start from what you implemented in task 4. Students are allowed to use pretrained networks, and utilize transfer learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Information: \n",
    "\n",
    "- Unzip zip files in GCP or acquire administrator permission for other application installation: When you upload your dataset to your vm instances, you may want to unzip your files. However, unzip command is not built in. To use `sudo apt install unzip` or for future applications installation, you need to: \n",
    "  - Change username to default (or just restart the vm instance)\n",
    "  - Type `sudo su` to get root\n",
    "  - You can remove sudo for the following installation commands (e.g. `apt install unzip`).\n",
    "- If you meet kernel crash (or the running never ends), you might consider using a larger memory CPU. Especially if you include large network structure like VGG, 15GB memory or more CPU is recommended\n",
    "- Some python libraries that you might need to install first: pandas, scikit-learn. there are **2 OPTIONS** that you can use to install them:\n",
    "  - In the envTF24 environment in linux interface, type: `pip install [package name]` \n",
    "  - In the jupyter notebook (i.e. this file), type `!pip install [package name]`. You’d better restart the virtual environment, even the instance to get these packages functional.\n",
    "- You might need extra pip libraries to handle dataset, include network, etc. You can follow step 3 to install them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Submission Details:\n",
    "\n",
    "There are two components to reporting the results of this task: \n",
    "\n",
    "**(A) Submission (up to 20 submissions each day) of the .csv prediction file through the Kaggle platform**. You should start doing this __VERY early__, so that students can compare their work as they are making progress with model optimization.\n",
    "\n",
    "**(B) Submitting your best CNN model through Github Classroom repo.**\n",
    "\n",
    "**Note** that assignments are submitted through github classroom only. All code for training your kaggle model should be done in this task 5 jupyter notebook, or in a user defined module (.py file) that is imported for use in the jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Submission content:__</span>\n",
    "\n",
    "(i) In your Assignment 2 submission folder, create a subfolder called __KaggleModel__. Save your best model using `model.save()`. This will generate a `saved_model.pb` file, a folder called `variables`, and a folder called `checkpoints` all inside the __KaggleModel__ folder. Only upload your best model to GitHub classroom. \n",
    "\n",
    "(ii) <span style=\"color:red\">If your saved model exceeds 100 MB, do not upload it to GitHub classroom (.gitignore it or you will get an error when pushing).</span> Upload it instead to Google Drive and explicitly provide the link under the 'Save your best model' cell. \n",
    "\n",
    "(iii) Remember to delete any intermediate results, we only want your best model. Do not upload any data files. The instructors will rerun the uploaded best model and verify against the score which you reported on the Kaggle.\n",
    "\n",
    "**The top 10 final submissions of the Kaggle competition will receive up to 10 bonus points proportional to the private test accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading folder 1\n",
      "Reading folder 3\n",
      "Reading folder 0\n",
      "Reading folder 2\n",
      "Reading folder 4\n",
      "Reading Test Images\n",
      "Training data shape:  (15000, 128, 128, 3)\n",
      "Training labels shape:  (15000,)\n",
      "Test data shape:  (3500, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "#Generate dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#Load Training images and labels\n",
    "train_directory = \"./data/train_128\" #TODO: Enter path for train128 folder (hint: use os.getcwd())\n",
    "image_list=[]\n",
    "label_list=[]\n",
    "for sub_dir in os.listdir(train_directory):\n",
    "    print(\"Reading folder {}\".format(sub_dir))\n",
    "    sub_dir_name=os.path.join(train_directory,sub_dir)\n",
    "    for file in os.listdir(sub_dir_name):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_list.append(np.array(Image.open(os.path.join(sub_dir_name,file))))\n",
    "            label_list.append(int(sub_dir))\n",
    "X_train=np.array(image_list)\n",
    "y_train=np.array(label_list)\n",
    "\n",
    "#Load Test images\n",
    "test_directory = \"./data/test_128\"#TODO: Enter path for test128 folder (hint: use os.getcwd())\n",
    "test_image_list=[]\n",
    "test_df = pd.DataFrame([], columns=['Id', 'X'])\n",
    "print(\"Reading Test Images\")\n",
    "for file in os.listdir(test_directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        test_df = test_df.append({\n",
    "            'Id': filename,\n",
    "            'X': np.array(Image.open(os.path.join(test_directory,file)))\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "test_df['s'] = [int(x.split('.')[0]) for x in test_df['Id']]\n",
    "test_df = test_df.sort_values(by=['s'])\n",
    "test_df = test_df.drop(columns=['s'])\n",
    "X_test = np.stack(test_df['X'])\n",
    "\n",
    "\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Your Model Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:17<00:00, 26.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4969576597213745, Accuracy: 96.1872329711914 \n",
      "Training Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.141601324081421, Accuracy: 25.88140869140625 \n",
      "Training Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4605143070220947, Accuracy: 31.477031707763672 \n",
      "Training Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3375842571258545, Accuracy: 39.83039474487305 \n",
      "Training Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.194035530090332, Accuracy: 48.103633880615234 \n",
      "Training Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0511714220046997, Accuracy: 56.52376937866211 \n",
      "Training Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9186573624610901, Accuracy: 64.44978332519531 \n",
      "Training Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7618063688278198, Accuracy: 72.32906341552734 \n",
      "Training Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6335304975509644, Accuracy: 77.57746124267578 \n",
      "Training Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5153590440750122, Accuracy: 82.1380844116211 \n",
      "Training Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.43831667304039, Accuracy: 85.16960906982422 \n",
      "Training Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3818196952342987, Accuracy: 87.44657897949219 \n",
      "Training Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.326097697019577, Accuracy: 89.20272827148438 \n",
      "Training Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.28427767753601074, Accuracy: 90.83867645263672 \n",
      "Training Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.25318560004234314, Accuracy: 92.14075469970703 \n",
      "Training Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.23114848136901855, Accuracy: 92.8619155883789 \n",
      "Training Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.18989714980125427, Accuracy: 93.85015869140625 \n",
      "Training Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.18283507227897644, Accuracy: 94.14396667480469 \n",
      "Training Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.16037869453430176, Accuracy: 94.90518188476562 \n",
      "Training Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.14772939682006836, Accuracy: 95.30582427978516 \n",
      "Training Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.13377350568771362, Accuracy: 95.69978332519531 \n",
      "Training Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.135394886136055, Accuracy: 95.73985290527344 \n",
      "Training Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.11268161237239838, Accuracy: 96.46768188476562 \n",
      "Training Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.10865306854248047, Accuracy: 96.45433044433594 \n",
      "Training Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.09885963797569275, Accuracy: 96.75480651855469 \n",
      "Training Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08907879143953323, Accuracy: 97.14209747314453 \n",
      "Training Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08400726318359375, Accuracy: 97.23558044433594 \n",
      "Training Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08263134211301804, Accuracy: 97.31570434570312 \n",
      "Training Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0834260955452919, Accuracy: 97.5494155883789 \n",
      "Training Epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:18<00:00, 25.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07064913213253021, Accuracy: 97.80315399169922 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D, Dropout , BatchNormalization,MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "from utils.image_generator import ImageGenerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MyModel(Model):\n",
    "\n",
    "    def __init__(self, input_shape, output_size=5):\n",
    "       \n",
    "        super(MyModel, self).__init__()\n",
    "        # For example:\n",
    "        \n",
    "        self.conv_layer_1 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1,1), activation='relu', input_shape=input_shape, padding=\"same\")\n",
    "        self.maxpool_layer_1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "        self.conv_layer_2 = Conv2D(filters=64, kernel_size=(3, 3), strides=(1,1), activation='relu', input_shape=input_shape, padding=\"same\")\n",
    "        self.maxpool_layer_2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "        self.conv_layer_3 = Conv2D(filters=128, kernel_size=(3, 3), strides=(1,1), activation='relu', input_shape=input_shape, padding=\"same\")\n",
    "        self.maxpool_layer_3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "        self.batch_norm_1 = BatchNormalization()\n",
    "        self.conv_layer_4 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1,1), activation='relu', input_shape=input_shape, padding=\"same\")\n",
    "        self.maxpool_layer_4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "        self.batch_norm_2 = BatchNormalization()\n",
    "        self.drop_out_1 = Dropout(0.5)\n",
    "        self.flatten_layer = Flatten()\n",
    "        self.fc_layer_1 = Dense(256, activation='relu')\n",
    "        self.drop_out_2 = Dropout(0.5)\n",
    "        self.fc_layer_2 = Dense(128,activation='relu')\n",
    "        self.fc_layer_3 = Dense(output_size, activation='softmax')      \n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.conv_layer_1(x)\n",
    "        x = self.maxpool_layer_1(x)\n",
    "        x = self.conv_layer_2(x)\n",
    "        x = self.maxpool_layer_2(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.maxpool_layer_3(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.conv_layer_4(x)\n",
    "        x = self.maxpool_layer_4(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.drop_out_1(x)\n",
    "        x = self.flatten_layer(x)\n",
    "        x = self.fc_layer_1(x)\n",
    "        x = self.drop_out_2(x)\n",
    "        x = self.fc_layer_2(x)\n",
    "        out = self.fc_layer_3(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class My_trainer():\n",
    "       \n",
    "    def __init__(self,X_train, y_train, X_test,epochs=10, batch_size=256, lr=1e-3):\n",
    "        self.X_train = X_train.astype(\"float32\")\n",
    "        self.y_train = y_train.astype(\"float32\")\n",
    "        self.X_test = X_test.astype(\"float32\")\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "    # Initialize MyLenet model\n",
    "    def init_model(self):\n",
    "        self.model = MyModel(self.X_train[0].shape)\n",
    "\n",
    "    #initialize loss function and metrics to track over training\n",
    "    def init_loss(self):\n",
    "        self.loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    # Initialize optimizer\n",
    "    def init_optimizer(self):\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "    # Prepare batches of train data using ImageGenerator\n",
    "    def batch_train_data(self, shuffle=True):\n",
    "        train_data = ImageGenerator(self.X_train, self.y_train)\n",
    "        self.train_data_next_batch = train_data.next_batch_gen(self.batch_size,shuffle=shuffle)\n",
    "        self.n_batches = train_data.N_aug // self.batch_size\n",
    "    \n",
    "    # Define training step\n",
    "    def train_step(self, images, labels, training=True):\n",
    "        with tf.GradientTape() as tape:\n",
    "        # training=True is always recommended as there are few layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "            predictions = self.model(images, training=training)\n",
    "            loss = self.loss_function(labels, predictions)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "        self.train_loss(loss)\n",
    "        self.train_accuracy(labels, predictions)\n",
    "\n",
    "    # train epoch\n",
    "    def train_epoch(self, epoch):\n",
    "        self.train_loss.reset_states()\n",
    "        self.train_accuracy.reset_states()\n",
    "        for batches in tqdm(range (self.n_batches)):\n",
    "            x_batch,y_batch = next(self.train_data_next_batch)\n",
    "            self.train_step(x_batch,y_batch)\n",
    "\n",
    "        template = 'Loss: {}, Accuracy: {} '\n",
    "        print(template.format(self.train_loss.result(),\n",
    "                            self.train_accuracy.result() * 100))\n",
    "                            \n",
    "            \n",
    "    # start training\n",
    "    def run(self):\n",
    "        self.init_model()\n",
    "        self.init_loss()\n",
    "        self.init_optimizer()\n",
    "        self.batch_train_data()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print('Training Epoch {}'.format(epoch + 1))\n",
    "            self.train_epoch(epoch)\n",
    "            \n",
    "    def predict(self):\n",
    "\n",
    "        predictions = self.model.call(self.X_test)\n",
    "        predictions = np.argmax(predictions,axis =1)\n",
    "\n",
    "        return predictions \n",
    "    \n",
    "Trainer = My_trainer(X_train,y_train,X_test,epochs= 30,batch_size=32,lr = 0.0005)\n",
    "Trainer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your best model\n",
    "\n",
    "**Link to large model on Google Drive: [insert link here]** (if model exceeds 100 MB, else upload to GitHub classroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/task5_model/assets\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "Trainer.model.save(filepath = \"./model/task5_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate .csv file for Kaggle\n",
    "\n",
    "The following code snippet can be an example used to generate your prediction .csv file.\n",
    "\n",
    "NOTE: If your Kaggle results are indicating random performance, then it's likely that the indices of your csv predictions are misaligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with tf.device('/CPU:0'):\n",
    "    predictions = Trainer.predict()\n",
    "with open('./model/task5_model/predictions_task_5.csv','w') as csvfile:\n",
    "    fieldnames = ['Id','label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for index,l in enumerate(predictions):\n",
    "        filename = str(index) + '.png'\n",
    "        label = str(l)\n",
    "        writer.writerow({'Id': filename, 'label': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
