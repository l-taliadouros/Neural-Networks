{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJCMCx6ISZZw"
   },
   "source": [
    "## **Task 3: RNN Application -- Neural Machine Translation** (25%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8wjQhNtSffW"
   },
   "source": [
    "In this task, you are going to perform neural machine translation (NMT). NMT involves using a neural network to translate from one language to another. This is a widely studied natural language processing (NLP) problem and has tremendous real-world applications.\n",
    "\n",
    "Machine Translation is a challenging task that involves both the usage of complex architectures and data processing tricks to obtain human-level performance. **In this notebook, you will implement a simple Seq2Seq architecture using RNN layers in keras.**\n",
    "\n",
    "**The goal is to train a model to translate from Dutch (input language) to English (target language)**. This notebook uses data from the [Tab Delimited Bilingual Sentence Pairs](https://www.manythings.org/anki/) repository. You can find many such language pairs here.\n",
    "\n",
    "## <span style=\"color:red\"><strong>NOTE: Training this model may take 10-15 minutes of time depending on the strength of the system, so please plan accordingly.</strong></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UZ_G4XdfP7GK"
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MjTYqMoN8fh"
   },
   "source": [
    "### Broad Overview of Steps:\n",
    "1. Preprocess and encode data\n",
    "2. Create dataset/dataloaders\n",
    "3. Define Model Architecture\n",
    "4. Train Model\n",
    "5. Evaluate results\n",
    "\n",
    "Step 1. has already been completed for you. We provide two .npy files that contain the data: \n",
    "- `nmt_eng.npy` contains the encoded English sentences.\n",
    "- `nmt_nl.npy` contains the encoded Dutch sentences. \n",
    "The sentences already have been normalized, padded and appended with the \\<start\\> and \\<end\\> tokens.\n",
    "\n",
    "We also provide two vocabulary files `eng_vocab.txt` and `nl_vocab.txt` for the English and Dutch languages respectively. The vocabulary files will be used for decoding the input and output of our model.\n",
    "\n",
    "## Part 1. Load Encoded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO:</strong></font> Execute the following cells to load the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F55EwI6RQl1A",
    "outputId": "1e087591-3dad-4471-97df-d9af9214dddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of english vocab: 9044\n",
      "Size of dutch vocab: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load Vocabulary files (dictionaries of word:int pairs)\n",
    "with open(\"text_data/eng_vocab.txt\", 'r') as f:\n",
    "    eng_vocab = json.load(f)\n",
    "\n",
    "with open(\"text_data/nl_vocab.txt\", 'r') as f:\n",
    "    nl_vocab = json.load(f)\n",
    "    \n",
    "eng_vocab = {int(key): value for key, value in eng_vocab.items()}\n",
    "nl_vocab = {int(key): value for key, value in nl_vocab.items()}\n",
    "\n",
    "print(f'Size of english vocab: {len(eng_vocab)}')\n",
    "print(f'Size of dutch vocab: {len(nl_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of english text data: (75298, 30)\n",
      "Shape of dutch text data: (75298, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load Encoded Sentence Data\n",
    "eng_text = np.load(\"text_data/nmt_eng.npy\")\n",
    "nl_text = np.load(\"text_data/nmt_nl.npy\")\n",
    "\n",
    "print(f'Shape of english text data: {eng_text.shape}')\n",
    "print(f'Shape of dutch text data: {nl_text.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmjy9sPDOCnY"
   },
   "source": [
    "## Part 2: Datasets and Dataloading (3%)\n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> <b>Complete the functions in utils/translation/text_data.py</b>\n",
    "\n",
    "This will create the train, validation, and test datasets for our translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-Mx6WgMBVI3T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 67768\n",
      "Validation size: 7530\n"
     ]
    }
   ],
   "source": [
    "from utils.translation.text_data import get_dataset, get_dataset_partitions_tf, decode_text\n",
    "\n",
    "text_ds = get_dataset(nl_text, eng_text)\n",
    "train_ds, val_ds = get_dataset_partitions_tf(text_ds, len(text_ds))\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Validation size: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
       " array([  2,  57,  82,  27,   9, 167,  62, 341,   7,   3,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0])>,\n",
       " <tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
       " array([  2,  28,  63,  47,   6,  50,   9, 319, 158,   8,   3,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0])>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at a sample from the dataset\n",
    "sample = next(iter(train_ds))\n",
    "sample[0], sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NL: ['[SOS]', 'hoe', 'laat', 'ben', 'je', 'gisteren', 'gaan', 'slapen', '?', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "EN: ['[SOS]', 'what', 'time', 'did', 'you', 'go', 'to', 'sleep', 'yesterday', '?', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "decoded_nl = decode_text(sample[0].numpy(), vocab=nl_vocab)\n",
    "decoded_eng = decode_text(sample[1].numpy(), vocab=eng_vocab)\n",
    "print('NL:', decoded_nl)\n",
    "print()\n",
    "print('EN:', decoded_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBiLRGc7RL-g"
   },
   "source": [
    "## Part 3: Model Architecture (15%)\n",
    "\n",
    "### Seq2Seq Model\n",
    "\n",
    "In the general case, input sequences and output sequences have different lengths (e.g. machine translation) and the entire input sequence is required in order to start predicting the target. This requires a more advanced setup, which is what people commonly refer to when mentioning \"sequence-to-sequence models\" with no further context. Here's how it works (This example is English to French):\n",
    "\n",
    "- An RNN layer (or stack thereof) acts as \"encoder\": it processes the input sequence and returns its own internal state. Note that we discard the outputs of the encoder RNN, only recovering the state. This state will serve as the \"context\", or \"conditioning\", of the decoder in the next step.\n",
    "- Another RNN layer (or stack thereof) acts as \"decoder\": it is trained to predict the next characters of the target sequence, given previous characters of the target sequence. Specifically, it is trained to turn the target sequences into the same sequences but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. Importantly, the encoder uses as initial state the state vectors from the encoder, which is how the decoder obtains information about what it is supposed to generate. Effectively, the decoder learns to generate targets[t+1...] given targets[...t], conditioned on the input sequence.\n",
    "\n",
    "![teacher_forcing](./img/seq2seq-teacher-forcing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In inference mode, i.e. when we want to decode unknown input sequences, we go through a slightly different process:\n",
    "\n",
    "1) Encode the input sequence into state vectors.\n",
    "2) Start with a target sequence of size 1 (just the start-of-sequence character).\n",
    "3) Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character.\n",
    "4) Sample the next character using these predictions (we simply use argmax).\n",
    "5) Append the sampled character to the target sequence\n",
    "6) Repeat until we generate the end-of-sequence character or we hit the character limit.\n",
    "\n",
    "![seq2seq-inference](./img/seq2seq-inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seq2seq model implementation requires a more complex setup than what is provided by keras.Sequential().\n",
    "You will be exposed to writing modular code using custom `keras.layer` and `keras.Model` classes. **First, please read https://keras.io/guides/making_new_layers_and_models_via_subclassing/** to get an idea about writing custom modules in tensorflow/keras, which is what is done in practice to implement complex architectures.\n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> <b>Based on the above, you need to complete the code in utils/translation/layers.py</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH, PREFETCH, CACHE the datasets\n",
    "# You can change the batch size based on memory requirements\n",
    "BATCH_SIZE = 64\n",
    "train_loader = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE).cache()\n",
    "val_loader = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.translation.layers import TranslationModel\n",
    "\n",
    "eng_vocab_size = len(eng_vocab)\n",
    "nl_vocab_size = len(nl_vocab)\n",
    "hidden_size = 256\n",
    "\n",
    "# Initialize Model\n",
    "model = TranslationModel(nl_vocab_size, eng_vocab_size, hidden_size, eng_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training the Model (5%)\n",
    "\n",
    "The following cell(s) will train your Machine Translation model. The loss function used is Cross Entropy (since we are performing classification across the vocabulary at each time step). In practice, we usually implement a machine translation metric such as BLEU or ROUGE ([reference](https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb#:~:text=While%20BLEU%20score%20is%20primarily,the%20reference%20translations%20or%20summaries.)), and compute it for the validation set after each epoch. For this task, it is sufficient to just observe the train loss values.\n",
    "\n",
    "You are already provided the `train_seq2seq_model` function in `utils.translation.train_funcs.py`. You can refer to this file to see the loss function and how a custom training loop with modifications has been implemented. Execute the cell below to train your model.\n",
    "\n",
    "**Note that training will proceed as expected only if the implementation of your model is correct.** You can monitor the training loss to make sure that the model training is proceeding as expected. **Training may take 10-15 minutes depending on the strength of the system.**\n",
    "\n",
    "If you have spare time, feel free to increase the number of epoch and gauge if the performance improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/8\n",
      "Iter: 0, Loss (iter): 9.11008358001709, Mean Loss (over last 50 iters): 9.11008358001709\n",
      "Iter: 50, Loss (iter): 5.750728607177734, Mean Loss (over last 50 iters): 6.467575550079346\n",
      "Iter: 100, Loss (iter): 5.2706170082092285, Mean Loss (over last 50 iters): 5.489830493927002\n",
      "Iter: 150, Loss (iter): 5.337277889251709, Mean Loss (over last 50 iters): 5.267693996429443\n",
      "Iter: 200, Loss (iter): 5.086941242218018, Mean Loss (over last 50 iters): 5.1465654373168945\n",
      "Iter: 250, Loss (iter): 5.174102783203125, Mean Loss (over last 50 iters): 5.066580772399902\n",
      "Iter: 300, Loss (iter): 4.774022579193115, Mean Loss (over last 50 iters): 4.946993350982666\n",
      "Iter: 350, Loss (iter): 4.859435558319092, Mean Loss (over last 50 iters): 4.828985214233398\n",
      "Iter: 400, Loss (iter): 4.607417106628418, Mean Loss (over last 50 iters): 4.692594051361084\n",
      "Iter: 450, Loss (iter): 4.604677677154541, Mean Loss (over last 50 iters): 4.642575263977051\n",
      "Iter: 500, Loss (iter): 4.480602741241455, Mean Loss (over last 50 iters): 4.547545909881592\n",
      "Iter: 550, Loss (iter): 4.514379978179932, Mean Loss (over last 50 iters): 4.447962760925293\n",
      "Iter: 600, Loss (iter): 4.410334587097168, Mean Loss (over last 50 iters): 4.434814929962158\n",
      "Iter: 650, Loss (iter): 4.318635940551758, Mean Loss (over last 50 iters): 4.349687576293945\n",
      "Iter: 700, Loss (iter): 4.416466236114502, Mean Loss (over last 50 iters): 4.290778160095215\n",
      "Iter: 750, Loss (iter): 4.098503112792969, Mean Loss (over last 50 iters): 4.2296648025512695\n",
      "Iter: 800, Loss (iter): 4.384649753570557, Mean Loss (over last 50 iters): 4.126438140869141\n",
      "Iter: 850, Loss (iter): 4.011294841766357, Mean Loss (over last 50 iters): 4.078524112701416\n",
      "Iter: 900, Loss (iter): 3.8656840324401855, Mean Loss (over last 50 iters): 3.983945846557617\n",
      "Iter: 950, Loss (iter): 4.05919885635376, Mean Loss (over last 50 iters): 3.958611488342285\n",
      "Iter: 1000, Loss (iter): 3.9815821647644043, Mean Loss (over last 50 iters): 3.8879053592681885\n",
      "Iter: 1050, Loss (iter): 3.934079885482788, Mean Loss (over last 50 iters): 3.8499529361724854\n",
      "Epoch: 2/8\n",
      "Iter: 0, Loss (iter): 3.6745615005493164, Mean Loss (over last 50 iters): 3.8253307342529297\n",
      "Iter: 50, Loss (iter): 3.881500244140625, Mean Loss (over last 50 iters): 3.8406546115875244\n",
      "Iter: 100, Loss (iter): 3.6562371253967285, Mean Loss (over last 50 iters): 3.7567379474639893\n",
      "Iter: 150, Loss (iter): 3.759000301361084, Mean Loss (over last 50 iters): 3.735464572906494\n",
      "Iter: 200, Loss (iter): 3.6679792404174805, Mean Loss (over last 50 iters): 3.6943182945251465\n",
      "Iter: 250, Loss (iter): 3.6794931888580322, Mean Loss (over last 50 iters): 3.6986701488494873\n",
      "Iter: 300, Loss (iter): 3.5287816524505615, Mean Loss (over last 50 iters): 3.633796453475952\n",
      "Iter: 350, Loss (iter): 3.6428308486938477, Mean Loss (over last 50 iters): 3.592006206512451\n",
      "Iter: 400, Loss (iter): 3.580664873123169, Mean Loss (over last 50 iters): 3.533390760421753\n",
      "Iter: 450, Loss (iter): 3.490067481994629, Mean Loss (over last 50 iters): 3.5386087894439697\n",
      "Iter: 500, Loss (iter): 3.4513723850250244, Mean Loss (over last 50 iters): 3.4953219890594482\n",
      "Iter: 550, Loss (iter): 3.500183343887329, Mean Loss (over last 50 iters): 3.4507176876068115\n",
      "Iter: 600, Loss (iter): 3.431881904602051, Mean Loss (over last 50 iters): 3.4592275619506836\n",
      "Iter: 650, Loss (iter): 3.324504852294922, Mean Loss (over last 50 iters): 3.41591215133667\n",
      "Iter: 700, Loss (iter): 3.439178466796875, Mean Loss (over last 50 iters): 3.388972759246826\n",
      "Iter: 750, Loss (iter): 3.138167142868042, Mean Loss (over last 50 iters): 3.343975782394409\n",
      "Iter: 800, Loss (iter): 3.506183385848999, Mean Loss (over last 50 iters): 3.290891647338867\n",
      "Iter: 850, Loss (iter): 3.180292844772339, Mean Loss (over last 50 iters): 3.305513381958008\n",
      "Iter: 900, Loss (iter): 3.1259827613830566, Mean Loss (over last 50 iters): 3.2421000003814697\n",
      "Iter: 950, Loss (iter): 3.3412911891937256, Mean Loss (over last 50 iters): 3.2401533126831055\n",
      "Iter: 1000, Loss (iter): 3.3045523166656494, Mean Loss (over last 50 iters): 3.2020959854125977\n",
      "Iter: 1050, Loss (iter): 3.2442214488983154, Mean Loss (over last 50 iters): 3.1669652462005615\n",
      "Epoch: 3/8\n",
      "Iter: 0, Loss (iter): 3.0305511951446533, Mean Loss (over last 50 iters): 3.1490392684936523\n",
      "Iter: 50, Loss (iter): 3.200653076171875, Mean Loss (over last 50 iters): 3.1881542205810547\n",
      "Iter: 100, Loss (iter): 3.008700370788574, Mean Loss (over last 50 iters): 3.112624406814575\n",
      "Iter: 150, Loss (iter): 3.1075692176818848, Mean Loss (over last 50 iters): 3.1154491901397705\n",
      "Iter: 200, Loss (iter): 3.0791802406311035, Mean Loss (over last 50 iters): 3.1089413166046143\n",
      "Iter: 250, Loss (iter): 3.082355499267578, Mean Loss (over last 50 iters): 3.1207141876220703\n",
      "Iter: 300, Loss (iter): 2.9962704181671143, Mean Loss (over last 50 iters): 3.0694210529327393\n",
      "Iter: 350, Loss (iter): 3.049614429473877, Mean Loss (over last 50 iters): 3.031656503677368\n",
      "Iter: 400, Loss (iter): 3.058385133743286, Mean Loss (over last 50 iters): 2.9833121299743652\n",
      "Iter: 450, Loss (iter): 3.01295804977417, Mean Loss (over last 50 iters): 2.991363286972046\n",
      "Iter: 500, Loss (iter): 2.9217100143432617, Mean Loss (over last 50 iters): 2.9641730785369873\n",
      "Iter: 550, Loss (iter): 2.9474079608917236, Mean Loss (over last 50 iters): 2.934574842453003\n",
      "Iter: 600, Loss (iter): 2.918492078781128, Mean Loss (over last 50 iters): 2.940502405166626\n",
      "Iter: 650, Loss (iter): 2.800889015197754, Mean Loss (over last 50 iters): 2.8956496715545654\n",
      "Iter: 700, Loss (iter): 2.9062340259552, Mean Loss (over last 50 iters): 2.8709781169891357\n",
      "Iter: 750, Loss (iter): 2.5967605113983154, Mean Loss (over last 50 iters): 2.8214080333709717\n",
      "Iter: 800, Loss (iter): 2.9605138301849365, Mean Loss (over last 50 iters): 2.7819836139678955\n",
      "Iter: 850, Loss (iter): 2.6528005599975586, Mean Loss (over last 50 iters): 2.7925832271575928\n",
      "Iter: 900, Loss (iter): 2.6294338703155518, Mean Loss (over last 50 iters): 2.736297369003296\n",
      "Iter: 950, Loss (iter): 2.818777084350586, Mean Loss (over last 50 iters): 2.7373197078704834\n",
      "Iter: 1000, Loss (iter): 2.8107824325561523, Mean Loss (over last 50 iters): 2.7101612091064453\n",
      "Iter: 1050, Loss (iter): 2.7993364334106445, Mean Loss (over last 50 iters): 2.687314033508301\n",
      "Epoch: 4/8\n",
      "Iter: 0, Loss (iter): 2.57674503326416, Mean Loss (over last 50 iters): 2.6681041717529297\n",
      "Iter: 50, Loss (iter): 2.7542505264282227, Mean Loss (over last 50 iters): 2.698241949081421\n",
      "Iter: 100, Loss (iter): 2.470046043395996, Mean Loss (over last 50 iters): 2.6216256618499756\n",
      "Iter: 150, Loss (iter): 2.616489887237549, Mean Loss (over last 50 iters): 2.6280758380889893\n",
      "Iter: 200, Loss (iter): 2.567181348800659, Mean Loss (over last 50 iters): 2.6170334815979004\n",
      "Iter: 250, Loss (iter): 2.593944787979126, Mean Loss (over last 50 iters): 2.6274428367614746\n",
      "Iter: 300, Loss (iter): 2.5146734714508057, Mean Loss (over last 50 iters): 2.5856432914733887\n",
      "Iter: 350, Loss (iter): 2.550997018814087, Mean Loss (over last 50 iters): 2.549513339996338\n",
      "Iter: 400, Loss (iter): 2.635049819946289, Mean Loss (over last 50 iters): 2.513313055038452\n",
      "Iter: 450, Loss (iter): 2.560882091522217, Mean Loss (over last 50 iters): 2.513239860534668\n",
      "Iter: 500, Loss (iter): 2.4624600410461426, Mean Loss (over last 50 iters): 2.4970362186431885\n",
      "Iter: 550, Loss (iter): 2.417766809463501, Mean Loss (over last 50 iters): 2.4739830493927\n",
      "Iter: 600, Loss (iter): 2.415546417236328, Mean Loss (over last 50 iters): 2.4756264686584473\n",
      "Iter: 650, Loss (iter): 2.3223719596862793, Mean Loss (over last 50 iters): 2.4349355697631836\n",
      "Iter: 700, Loss (iter): 2.4545271396636963, Mean Loss (over last 50 iters): 2.420727252960205\n",
      "Iter: 750, Loss (iter): 2.1658849716186523, Mean Loss (over last 50 iters): 2.3715929985046387\n",
      "Iter: 800, Loss (iter): 2.4734911918640137, Mean Loss (over last 50 iters): 2.340181589126587\n",
      "Iter: 850, Loss (iter): 2.2115564346313477, Mean Loss (over last 50 iters): 2.355294942855835\n",
      "Iter: 900, Loss (iter): 2.2201809883117676, Mean Loss (over last 50 iters): 2.304302453994751\n",
      "Iter: 950, Loss (iter): 2.3581018447875977, Mean Loss (over last 50 iters): 2.304699659347534\n",
      "Iter: 1000, Loss (iter): 2.3484678268432617, Mean Loss (over last 50 iters): 2.2904982566833496\n",
      "Iter: 1050, Loss (iter): 2.3759677410125732, Mean Loss (over last 50 iters): 2.2696361541748047\n",
      "Epoch: 5/8\n",
      "Iter: 0, Loss (iter): 2.200839042663574, Mean Loss (over last 50 iters): 2.2525763511657715\n",
      "Iter: 50, Loss (iter): 2.3091306686401367, Mean Loss (over last 50 iters): 2.2786307334899902\n",
      "Iter: 100, Loss (iter): 2.038175344467163, Mean Loss (over last 50 iters): 2.201977014541626\n",
      "Iter: 150, Loss (iter): 2.2257678508758545, Mean Loss (over last 50 iters): 2.2205498218536377\n",
      "Iter: 200, Loss (iter): 2.152179002761841, Mean Loss (over last 50 iters): 2.209930181503296\n",
      "Iter: 250, Loss (iter): 2.173548698425293, Mean Loss (over last 50 iters): 2.2237658500671387\n",
      "Iter: 300, Loss (iter): 2.107396364212036, Mean Loss (over last 50 iters): 2.183136224746704\n",
      "Iter: 350, Loss (iter): 2.1533877849578857, Mean Loss (over last 50 iters): 2.145564556121826\n",
      "Iter: 400, Loss (iter): 2.280449628829956, Mean Loss (over last 50 iters): 2.124155282974243\n",
      "Iter: 450, Loss (iter): 2.215590715408325, Mean Loss (over last 50 iters): 2.1316847801208496\n",
      "Iter: 500, Loss (iter): 2.112534999847412, Mean Loss (over last 50 iters): 2.119044065475464\n",
      "Iter: 550, Loss (iter): 2.007249593734741, Mean Loss (over last 50 iters): 2.0996313095092773\n",
      "Iter: 600, Loss (iter): 2.0239267349243164, Mean Loss (over last 50 iters): 2.098745822906494\n",
      "Iter: 650, Loss (iter): 1.9811925888061523, Mean Loss (over last 50 iters): 2.055614709854126\n",
      "Iter: 700, Loss (iter): 2.05403470993042, Mean Loss (over last 50 iters): 2.049960136413574\n",
      "Iter: 750, Loss (iter): 1.8336126804351807, Mean Loss (over last 50 iters): 2.0031814575195312\n",
      "Iter: 800, Loss (iter): 2.105475664138794, Mean Loss (over last 50 iters): 1.9816349744796753\n",
      "Iter: 850, Loss (iter): 1.8514275550842285, Mean Loss (over last 50 iters): 1.9973586797714233\n",
      "Iter: 900, Loss (iter): 1.898059606552124, Mean Loss (over last 50 iters): 1.9525607824325562\n",
      "Iter: 950, Loss (iter): 1.9617100954055786, Mean Loss (over last 50 iters): 1.9537506103515625\n",
      "Iter: 1000, Loss (iter): 1.9964275360107422, Mean Loss (over last 50 iters): 1.9470651149749756\n",
      "Iter: 1050, Loss (iter): 2.017425298690796, Mean Loss (over last 50 iters): 1.9253567457199097\n",
      "Epoch: 6/8\n",
      "Iter: 0, Loss (iter): 1.8736486434936523, Mean Loss (over last 50 iters): 1.9065757989883423\n",
      "Iter: 50, Loss (iter): 1.9654769897460938, Mean Loss (over last 50 iters): 1.9250364303588867\n",
      "Iter: 100, Loss (iter): 1.7126376628875732, Mean Loss (over last 50 iters): 1.8559668064117432\n",
      "Iter: 150, Loss (iter): 1.902561068534851, Mean Loss (over last 50 iters): 1.8816239833831787\n",
      "Iter: 200, Loss (iter): 1.8029124736785889, Mean Loss (over last 50 iters): 1.8709012269973755\n",
      "Iter: 250, Loss (iter): 1.8174978494644165, Mean Loss (over last 50 iters): 1.8821405172348022\n",
      "Iter: 300, Loss (iter): 1.763968825340271, Mean Loss (over last 50 iters): 1.8444390296936035\n",
      "Iter: 350, Loss (iter): 1.7990401983261108, Mean Loss (over last 50 iters): 1.8136242628097534\n",
      "Iter: 400, Loss (iter): 1.940287709236145, Mean Loss (over last 50 iters): 1.791739821434021\n",
      "Iter: 450, Loss (iter): 1.9009666442871094, Mean Loss (over last 50 iters): 1.7982522249221802\n",
      "Iter: 500, Loss (iter): 1.812045693397522, Mean Loss (over last 50 iters): 1.7874078750610352\n",
      "Iter: 550, Loss (iter): 1.674741506576538, Mean Loss (over last 50 iters): 1.7764185667037964\n",
      "Iter: 600, Loss (iter): 1.7088112831115723, Mean Loss (over last 50 iters): 1.7767871618270874\n",
      "Iter: 650, Loss (iter): 1.6606286764144897, Mean Loss (over last 50 iters): 1.7345404624938965\n",
      "Iter: 700, Loss (iter): 1.7284600734710693, Mean Loss (over last 50 iters): 1.727876901626587\n",
      "Iter: 750, Loss (iter): 1.5042283535003662, Mean Loss (over last 50 iters): 1.6842514276504517\n",
      "Iter: 800, Loss (iter): 1.7841304540634155, Mean Loss (over last 50 iters): 1.6676230430603027\n",
      "Iter: 850, Loss (iter): 1.53191339969635, Mean Loss (over last 50 iters): 1.686630129814148\n",
      "Iter: 900, Loss (iter): 1.6008880138397217, Mean Loss (over last 50 iters): 1.6461235284805298\n",
      "Iter: 950, Loss (iter): 1.6384456157684326, Mean Loss (over last 50 iters): 1.6447627544403076\n",
      "Iter: 1000, Loss (iter): 1.7053155899047852, Mean Loss (over last 50 iters): 1.6536991596221924\n",
      "Iter: 1050, Loss (iter): 1.7124117612838745, Mean Loss (over last 50 iters): 1.629197120666504\n",
      "Epoch: 7/8\n",
      "Iter: 0, Loss (iter): 1.575382113456726, Mean Loss (over last 50 iters): 1.614896297454834\n",
      "Iter: 50, Loss (iter): 1.6494338512420654, Mean Loss (over last 50 iters): 1.6235439777374268\n",
      "Iter: 100, Loss (iter): 1.4355080127716064, Mean Loss (over last 50 iters): 1.5528408288955688\n",
      "Iter: 150, Loss (iter): 1.6083446741104126, Mean Loss (over last 50 iters): 1.5896689891815186\n",
      "Iter: 200, Loss (iter): 1.4729689359664917, Mean Loss (over last 50 iters): 1.5686405897140503\n",
      "Iter: 250, Loss (iter): 1.5089561939239502, Mean Loss (over last 50 iters): 1.5851582288742065\n",
      "Iter: 300, Loss (iter): 1.4744833707809448, Mean Loss (over last 50 iters): 1.5467846393585205\n",
      "Iter: 350, Loss (iter): 1.5090123414993286, Mean Loss (over last 50 iters): 1.5260087251663208\n",
      "Iter: 400, Loss (iter): 1.6662663221359253, Mean Loss (over last 50 iters): 1.5092846155166626\n",
      "Iter: 450, Loss (iter): 1.5912846326828003, Mean Loss (over last 50 iters): 1.5133140087127686\n",
      "Iter: 500, Loss (iter): 1.5448533296585083, Mean Loss (over last 50 iters): 1.5061911344528198\n",
      "Iter: 550, Loss (iter): 1.3807849884033203, Mean Loss (over last 50 iters): 1.496730089187622\n",
      "Iter: 600, Loss (iter): 1.4332399368286133, Mean Loss (over last 50 iters): 1.4946191310882568\n",
      "Iter: 650, Loss (iter): 1.4032950401306152, Mean Loss (over last 50 iters): 1.4616564512252808\n",
      "Iter: 700, Loss (iter): 1.4349037408828735, Mean Loss (over last 50 iters): 1.4484219551086426\n",
      "Iter: 750, Loss (iter): 1.2186781167984009, Mean Loss (over last 50 iters): 1.4130500555038452\n",
      "Iter: 800, Loss (iter): 1.497475028038025, Mean Loss (over last 50 iters): 1.399653434753418\n",
      "Iter: 850, Loss (iter): 1.2979265451431274, Mean Loss (over last 50 iters): 1.4241544008255005\n",
      "Iter: 900, Loss (iter): 1.3637783527374268, Mean Loss (over last 50 iters): 1.3895392417907715\n",
      "Iter: 950, Loss (iter): 1.3662322759628296, Mean Loss (over last 50 iters): 1.3869816064834595\n",
      "Iter: 1000, Loss (iter): 1.450368881225586, Mean Loss (over last 50 iters): 1.3997503519058228\n",
      "Iter: 1050, Loss (iter): 1.4425544738769531, Mean Loss (over last 50 iters): 1.3801090717315674\n",
      "Epoch: 8/8\n",
      "Iter: 0, Loss (iter): 1.3422247171401978, Mean Loss (over last 50 iters): 1.3703771829605103\n",
      "Iter: 50, Loss (iter): 1.3479912281036377, Mean Loss (over last 50 iters): 1.3713008165359497\n",
      "Iter: 100, Loss (iter): 1.2051156759262085, Mean Loss (over last 50 iters): 1.3071043491363525\n",
      "Iter: 150, Loss (iter): 1.3234338760375977, Mean Loss (over last 50 iters): 1.3368573188781738\n",
      "Iter: 200, Loss (iter): 1.2128751277923584, Mean Loss (over last 50 iters): 1.3174399137496948\n",
      "Iter: 250, Loss (iter): 1.252816915512085, Mean Loss (over last 50 iters): 1.334165334701538\n",
      "Iter: 300, Loss (iter): 1.2308017015457153, Mean Loss (over last 50 iters): 1.2986184358596802\n",
      "Iter: 350, Loss (iter): 1.2893331050872803, Mean Loss (over last 50 iters): 1.2810745239257812\n",
      "Iter: 400, Loss (iter): 1.4302160739898682, Mean Loss (over last 50 iters): 1.2690918445587158\n",
      "Iter: 450, Loss (iter): 1.3565791845321655, Mean Loss (over last 50 iters): 1.2728793621063232\n",
      "Iter: 500, Loss (iter): 1.2980237007141113, Mean Loss (over last 50 iters): 1.2716784477233887\n",
      "Iter: 550, Loss (iter): 1.162906527519226, Mean Loss (over last 50 iters): 1.268340826034546\n",
      "Iter: 600, Loss (iter): 1.2371245622634888, Mean Loss (over last 50 iters): 1.2600983381271362\n",
      "Iter: 650, Loss (iter): 1.1580276489257812, Mean Loss (over last 50 iters): 1.2387043237686157\n",
      "Iter: 700, Loss (iter): 1.2039438486099243, Mean Loss (over last 50 iters): 1.216211199760437\n",
      "Iter: 750, Loss (iter): 0.9917314648628235, Mean Loss (over last 50 iters): 1.1942096948623657\n",
      "Iter: 800, Loss (iter): 1.2490729093551636, Mean Loss (over last 50 iters): 1.179872751235962\n",
      "Iter: 850, Loss (iter): 1.0832666158676147, Mean Loss (over last 50 iters): 1.2052189111709595\n",
      "Iter: 900, Loss (iter): 1.1566860675811768, Mean Loss (over last 50 iters): 1.1752712726593018\n",
      "Iter: 950, Loss (iter): 1.115781545639038, Mean Loss (over last 50 iters): 1.1744815111160278\n",
      "Iter: 1000, Loss (iter): 1.233375072479248, Mean Loss (over last 50 iters): 1.191965937614441\n",
      "Iter: 1050, Loss (iter): 1.218691110610962, Mean Loss (over last 50 iters): 1.1710693836212158\n"
     ]
    }
   ],
   "source": [
    "from utils.translation.train_funcs import train_seq2seq_model\n",
    "\n",
    "# Train the model. Use the Adam optimizer with 1e-3 learning rate.\n",
    "num_epochs = 8\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "train_seq2seq_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    num_epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluating Results (2%)\n",
    "\n",
    "Our training function only shows the training loss value. To assess the performance of the model, we can perform some predictions and decode the input/output sentences. \n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> Run the following cells to qualitatively asses the quality of the generated sentences and the performance of the trained model.\n",
    "\n",
    "**NOTE**: As we are dealing with a generation task, the outputs will vary depending on the final trained model. Therefore, we have provided a set of example outputs with the translation quality you can expect from the trained model. You results may be different.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these cells to evaluate your model on one batch of the validation set\n",
    "val_sample = val_loader.shuffle(10000).take(1)\n",
    "val_sample = next(iter(val_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 1\n",
      "Dutch Sentence:  mijn vrienden gingen zonder mij naar de film\n",
      "English Sentence (Truth):  my friends went to the movies without me\n",
      "English Sentence (Pred) my parents went to the movies without my cousin\n",
      "Sample: 2\n",
      "Dutch Sentence:  ik kan autorijden\n",
      "English Sentence (Truth):  i am able to drive a car\n",
      "English Sentence (Pred) i can drive a car\n",
      "Sample: 3\n",
      "Dutch Sentence:  ik wil je bezoeken\n",
      "English Sentence (Truth):  i want to visit you\n",
      "English Sentence (Pred) i want to see you\n",
      "Sample: 4\n",
      "Dutch Sentence:  ik probeerde te ontspannen maar dat lukte niet\n",
      "English Sentence (Truth):  i tried to relax but couldn t\n",
      "English Sentence (Pred) i tried to hear that i didn t see anything\n",
      "Sample: 5\n",
      "Dutch Sentence:  ik bel je morgen\n",
      "English Sentence (Truth):  i ll call you tomorrow\n",
      "English Sentence (Pred) i ll call you tomorrow\n",
      "Sample: 6\n",
      "Dutch Sentence:  je zult er op tijd aankomen zolang je tenminste de trein niet mist\n",
      "English Sentence (Truth):  you ll get there in time so long as you don t miss the train\n",
      "English Sentence (Pred) you ll be able to tell us about this hotel by the end of the time\n",
      "Sample: 7\n",
      "Dutch Sentence:  ik zou graag drie plaatsen willen reserveren\n",
      "English Sentence (Truth):  i d like to book three seats\n",
      "English Sentence (Pred) i d like to live in the city\n",
      "Sample: 8\n",
      "Dutch Sentence:  ik stel me [UNK]\n",
      "English Sentence (Truth):  i m running for office\n",
      "English Sentence (Pred) i like listening to me\n",
      "Sample: 9\n",
      "Dutch Sentence:  je hebt een lange dag gehad\n",
      "English Sentence (Truth):  you had a long day\n",
      "English Sentence (Pred) you had a long time ago\n",
      "Sample: 10\n",
      "Dutch Sentence:  tom spoelde de borden af en zette ze in de vaatwasser\n",
      "English Sentence (Truth):  tom rinsed off the plates and put them into the dishwasher\n",
      "English Sentence (Pred) tom rinsed off the plates and put it in the vase\n",
      "Sample: 11\n",
      "Dutch Sentence:  ze is irritant en egoistisch\n",
      "English Sentence (Truth):  she s annoying and selfish\n",
      "English Sentence (Pred) she s bad and selfish\n"
     ]
    }
   ],
   "source": [
    "val_inp, val_target = val_sample\n",
    "decoded_inputs = []\n",
    "reserved_tokens = ['[PAD]', '[SOS]', '[EOS]']\n",
    "for inp in val_inp.numpy():\n",
    "    decoded_text = decode_text(inp, vocab=nl_vocab)\n",
    "    decoded_inputs.append([token for token in decoded_text if token not in reserved_tokens])\n",
    "\n",
    "val_pred = model(val_inp, training=False)\n",
    "decoded_outputs = model.decode_tokens(val_pred)\n",
    "decoded_ground_truth = model.decode_tokens(val_target[:, 1:])\n",
    "\n",
    "samples_to_show = 10 # Should be <= batch size\n",
    "\n",
    "for i, decoded_data in enumerate(zip(decoded_inputs, decoded_ground_truth, decoded_outputs)):\n",
    "    nl_sentence = ' '.join(decoded_data[0])\n",
    "    gt_en_sentence = ' '.join(decoded_data[1])\n",
    "    pred_en_sentence = ' '.join(decoded_data[2])\n",
    "    print('Sample:', i+1)\n",
    "    print('Dutch Sentence: ', nl_sentence)\n",
    "    print('English Sentence (Truth): ', gt_en_sentence)\n",
    "    print('English Sentence (Pred)', pred_en_sentence)\n",
    "    if i > samples_to_show - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Expected Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSample: 1\\nDutch Sentence:  waar hebben jullie het verstopt ?\\nEnglish Sentence (Truth):  where did you hide it ?\\nEnglish Sentence (Pred) where did you do it ?\\nSample: 2\\nDutch Sentence:  denk jij dit ?\\nEnglish Sentence (Truth):  is that what you think ?\\nEnglish Sentence (Pred) do you think this ?\\nSample: 3\\nDutch Sentence:  de treinen rijden s nachts minder vaak\\nEnglish Sentence (Truth):  the trains don t run as often at night\\nEnglish Sentence (Pred) the [UNK] [UNK] to [UNK] a week\\nSample: 4\\nDutch Sentence:  ik weet hoe dit werkt\\nEnglish Sentence (Truth):  i know how this works\\nEnglish Sentence (Pred) i know that this dictionary\\nSample: 5\\nDutch Sentence:  houd je toespraak kort\\nEnglish Sentence (Truth):  keep your speech short\\nEnglish Sentence (Pred) look at your country\\nSample: 6\\nDutch Sentence:  help me alsjeblieft een trui uit te kiezen die bij mijn nieuwe jurk past\\nEnglish Sentence (Truth):  please help me pick out a sweater which matches my new dress\\nEnglish Sentence (Pred) please give me a new dictionary for me this year ago\\nSample: 7\\nDutch Sentence:  welk jaar is het ?\\nEnglish Sentence (Truth):  what year is it ?\\nEnglish Sentence (Pred) what is the last ?\\nSample: 8\\nDutch Sentence:  welk verschil is er tussen dit en dat ?\\nEnglish Sentence (Truth):  what is the difference between this and that ?\\nEnglish Sentence (Pred) what s the difference between this bird ?\\nSample: 9\\nDutch Sentence:  is dat onze bus ?\\nEnglish Sentence (Truth):  is that our bus ?\\nEnglish Sentence (Pred) is this the book ?\\nSample: 10\\nDutch Sentence:  kun je me vannacht een [UNK] doen en op mijn kinderen oppassen ?\\nEnglish Sentence (Truth):  could you do me a favor and [UNK] my kids tonight ?\\nEnglish Sentence (Pred) can you please tell the truth to me a doctor ?\\nSample: 11\\nDutch Sentence:  ik bleef daar\\nEnglish Sentence (Truth):  i stayed there\\nEnglish Sentence (Pred) i felt [UNK]\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample: 1\n",
    "Dutch Sentence:  waar hebben jullie het verstopt ?\n",
    "English Sentence (Truth):  where did you hide it ?\n",
    "English Sentence (Pred) where did you do it ?\n",
    "Sample: 2\n",
    "Dutch Sentence:  denk jij dit ?\n",
    "English Sentence (Truth):  is that what you think ?\n",
    "English Sentence (Pred) do you think this ?\n",
    "Sample: 3\n",
    "Dutch Sentence:  de treinen rijden s nachts minder vaak\n",
    "English Sentence (Truth):  the trains don t run as often at night\n",
    "English Sentence (Pred) the [UNK] [UNK] to [UNK] a week\n",
    "Sample: 4\n",
    "Dutch Sentence:  ik weet hoe dit werkt\n",
    "English Sentence (Truth):  i know how this works\n",
    "English Sentence (Pred) i know that this dictionary\n",
    "Sample: 5\n",
    "Dutch Sentence:  houd je toespraak kort\n",
    "English Sentence (Truth):  keep your speech short\n",
    "English Sentence (Pred) look at your country\n",
    "Sample: 6\n",
    "Dutch Sentence:  help me alsjeblieft een trui uit te kiezen die bij mijn nieuwe jurk past\n",
    "English Sentence (Truth):  please help me pick out a sweater which matches my new dress\n",
    "English Sentence (Pred) please give me a new dictionary for me this year ago\n",
    "Sample: 7\n",
    "Dutch Sentence:  welk jaar is het ?\n",
    "English Sentence (Truth):  what year is it ?\n",
    "English Sentence (Pred) what is the last ?\n",
    "Sample: 8\n",
    "Dutch Sentence:  welk verschil is er tussen dit en dat ?\n",
    "English Sentence (Truth):  what is the difference between this and that ?\n",
    "English Sentence (Pred) what s the difference between this bird ?\n",
    "Sample: 9\n",
    "Dutch Sentence:  is dat onze bus ?\n",
    "English Sentence (Truth):  is that our bus ?\n",
    "English Sentence (Pred) is this the book ?\n",
    "Sample: 10\n",
    "Dutch Sentence:  kun je me vannacht een [UNK] doen en op mijn kinderen oppassen ?\n",
    "English Sentence (Truth):  could you do me a favor and [UNK] my kids tonight ?\n",
    "English Sentence (Pred) can you please tell the truth to me a doctor ?\n",
    "Sample: 11\n",
    "Dutch Sentence:  ik bleef daar\n",
    "English Sentence (Truth):  i stayed there\n",
    "English Sentence (Pred) i felt [UNK]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"><strong>TODO:</strong></font> <b>Answer the following questions:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Describe your observations of the model's evaluation performance. Briefly explain any one method to improve the model architecture based on the lecture readings, or online sources.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Answer:__</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having increased the number of epochs, model's perfomance was pretty accurate, managing to produce an output similar to the ones in the expected output. A technique to further increase, based on online resources, would be to introduce an attention mechanism (https://medium.com/@prakhargannu/attention-mechanism-in-deep-learning-simplified-d6a5830a079d), that would give the power to the model to concentrate on specific parts of the input sequence dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **During the data preprocessing, we encoded each word in the input/target language to a number based on the vocabulary. This is known as tokenization. Briefly explain any one other method of tokenization, and why it might be beneficial to this particular task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Answer:__</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a technique could be subword tokenization (https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c). In our case it could be highly beneficial, as it could handle rare and out of vocabulary words, improve generalization and translation quality. (I also provided the reference, hence i think you do not need a lenghtier response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a simple LSTM-based seq2seq model. The performance may not be the best, since Dutch and English are naturally complex languages. The state-of-the-art translation models are based on Transformer Networks that use the attention mechanism. (further reading: https://nlpprogress.com/english/machine_translation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BONUS) Part 6: Bidirectional LSTM (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple modification that we can do to significantly improve the quality of the generated sentences is to change the Encoder LSTM to be bidirectional. This will improve the performance because different languages tend to have different sentence structures, and in the case of Dutch, crucial information for a given word may not be available until later on in the sentence. \n",
    "\n",
    "**SIDE NOTE**: Changing the decoder to be bidirectional will not work in a text generation task (in our case, translation). Feel free to think of why this is the case. (No writing is required).\n",
    "\n",
    "<font color=\"red\"><strong>BONUS TODO:</strong></font> <b> Complete the BidirectionalEncoder Class in utils/translation/layers.py, and run the training and validation loops to compare the generated translations with the previous model you implemented.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.translation.layers import TranslationModel\n",
    "\n",
    "eng_vocab_size = len(eng_vocab)\n",
    "nl_vocab_size = len(nl_vocab)\n",
    "hidden_size = 256\n",
    "\n",
    "# Initialize Model with bidirectional_encoder = True\n",
    "# NOTE the bidirectional_encoder = True\n",
    "model = TranslationModel(nl_vocab_size, eng_vocab_size, hidden_size, eng_vocab, bidirectional_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/8\n",
      "Iter: 0, Loss (iter): 5.482997894287109, Mean Loss (over last 50 iters): 5.482997894287109\n",
      "Iter: 50, Loss (iter): 5.286454200744629, Mean Loss (over last 50 iters): 5.355573654174805\n",
      "Iter: 100, Loss (iter): 5.019895553588867, Mean Loss (over last 50 iters): 5.1938700675964355\n",
      "Iter: 150, Loss (iter): 5.138948917388916, Mean Loss (over last 50 iters): 5.054823398590088\n",
      "Iter: 200, Loss (iter): 4.7865447998046875, Mean Loss (over last 50 iters): 4.873647689819336\n",
      "Iter: 250, Loss (iter): 4.744236946105957, Mean Loss (over last 50 iters): 4.716247081756592\n",
      "Iter: 300, Loss (iter): 4.362544059753418, Mean Loss (over last 50 iters): 4.543221950531006\n",
      "Iter: 350, Loss (iter): 4.470834255218506, Mean Loss (over last 50 iters): 4.418442726135254\n",
      "Iter: 400, Loss (iter): 4.184449195861816, Mean Loss (over last 50 iters): 4.2786478996276855\n",
      "Iter: 450, Loss (iter): 4.202212333679199, Mean Loss (over last 50 iters): 4.2479448318481445\n",
      "Iter: 500, Loss (iter): 4.092261791229248, Mean Loss (over last 50 iters): 4.160831928253174\n",
      "Iter: 550, Loss (iter): 4.131557941436768, Mean Loss (over last 50 iters): 4.072613716125488\n",
      "Iter: 600, Loss (iter): 4.072437286376953, Mean Loss (over last 50 iters): 4.0844035148620605\n",
      "Iter: 650, Loss (iter): 3.8941707611083984, Mean Loss (over last 50 iters): 4.018894195556641\n",
      "Iter: 700, Loss (iter): 4.114620208740234, Mean Loss (over last 50 iters): 3.966060161590576\n",
      "Iter: 750, Loss (iter): 3.7481272220611572, Mean Loss (over last 50 iters): 3.896573543548584\n",
      "Iter: 800, Loss (iter): 4.094830513000488, Mean Loss (over last 50 iters): 3.8404006958007812\n",
      "Iter: 850, Loss (iter): 3.726586103439331, Mean Loss (over last 50 iters): 3.82707142829895\n",
      "Iter: 900, Loss (iter): 3.6332015991210938, Mean Loss (over last 50 iters): 3.7648046016693115\n",
      "Iter: 950, Loss (iter): 3.8357768058776855, Mean Loss (over last 50 iters): 3.7452683448791504\n",
      "Iter: 1000, Loss (iter): 3.7473742961883545, Mean Loss (over last 50 iters): 3.685835599899292\n",
      "Iter: 1050, Loss (iter): 3.751673460006714, Mean Loss (over last 50 iters): 3.658433198928833\n",
      "Epoch: 2/8\n",
      "Iter: 0, Loss (iter): 3.4725842475891113, Mean Loss (over last 50 iters): 3.6360132694244385\n",
      "Iter: 50, Loss (iter): 3.666106700897217, Mean Loss (over last 50 iters): 3.6420764923095703\n",
      "Iter: 100, Loss (iter): 3.469431161880493, Mean Loss (over last 50 iters): 3.577451229095459\n",
      "Iter: 150, Loss (iter): 3.581066846847534, Mean Loss (over last 50 iters): 3.584845542907715\n",
      "Iter: 200, Loss (iter): 3.516378164291382, Mean Loss (over last 50 iters): 3.5573766231536865\n",
      "Iter: 250, Loss (iter): 3.5875906944274902, Mean Loss (over last 50 iters): 3.5661046504974365\n",
      "Iter: 300, Loss (iter): 3.4431686401367188, Mean Loss (over last 50 iters): 3.51570200920105\n",
      "Iter: 350, Loss (iter): 3.5311079025268555, Mean Loss (over last 50 iters): 3.484609603881836\n",
      "Iter: 400, Loss (iter): 3.485663414001465, Mean Loss (over last 50 iters): 3.440034866333008\n",
      "Iter: 450, Loss (iter): 3.396981716156006, Mean Loss (over last 50 iters): 3.445664167404175\n",
      "Iter: 500, Loss (iter): 3.365226984024048, Mean Loss (over last 50 iters): 3.4191641807556152\n",
      "Iter: 550, Loss (iter): 3.399585008621216, Mean Loss (over last 50 iters): 3.379948139190674\n",
      "Iter: 600, Loss (iter): 3.345332622528076, Mean Loss (over last 50 iters): 3.391643762588501\n",
      "Iter: 650, Loss (iter): 3.2595624923706055, Mean Loss (over last 50 iters): 3.360898971557617\n",
      "Iter: 700, Loss (iter): 3.4972944259643555, Mean Loss (over last 50 iters): 3.3380794525146484\n",
      "Iter: 750, Loss (iter): 3.105360269546509, Mean Loss (over last 50 iters): 3.292318105697632\n",
      "Iter: 800, Loss (iter): 3.474364995956421, Mean Loss (over last 50 iters): 3.2650279998779297\n",
      "Iter: 850, Loss (iter): 3.138392448425293, Mean Loss (over last 50 iters): 3.269680976867676\n",
      "Iter: 900, Loss (iter): 3.0849287509918213, Mean Loss (over last 50 iters): 3.2192633152008057\n",
      "Iter: 950, Loss (iter): 3.2247719764709473, Mean Loss (over last 50 iters): 3.201772451400757\n",
      "Iter: 1000, Loss (iter): 3.228442668914795, Mean Loss (over last 50 iters): 3.182657480239868\n",
      "Iter: 1050, Loss (iter): 3.201613426208496, Mean Loss (over last 50 iters): 3.1560559272766113\n",
      "Epoch: 3/8\n",
      "Iter: 0, Loss (iter): 3.0583159923553467, Mean Loss (over last 50 iters): 3.140214443206787\n",
      "Iter: 50, Loss (iter): 3.176736354827881, Mean Loss (over last 50 iters): 3.150264263153076\n",
      "Iter: 100, Loss (iter): 2.9422242641448975, Mean Loss (over last 50 iters): 3.0835909843444824\n",
      "Iter: 150, Loss (iter): 3.061732053756714, Mean Loss (over last 50 iters): 3.0953779220581055\n",
      "Iter: 200, Loss (iter): 3.003530502319336, Mean Loss (over last 50 iters): 3.0766594409942627\n",
      "Iter: 250, Loss (iter): 3.1080446243286133, Mean Loss (over last 50 iters): 3.08115816116333\n",
      "Iter: 300, Loss (iter): 2.95591139793396, Mean Loss (over last 50 iters): 3.04622745513916\n",
      "Iter: 350, Loss (iter): 3.041940450668335, Mean Loss (over last 50 iters): 3.0111920833587646\n",
      "Iter: 400, Loss (iter): 3.084555149078369, Mean Loss (over last 50 iters): 2.970876693725586\n",
      "Iter: 450, Loss (iter): 2.9521901607513428, Mean Loss (over last 50 iters): 2.9786858558654785\n",
      "Iter: 500, Loss (iter): 2.932459592819214, Mean Loss (over last 50 iters): 2.9544677734375\n",
      "Iter: 550, Loss (iter): 2.8967909812927246, Mean Loss (over last 50 iters): 2.933154344558716\n",
      "Iter: 600, Loss (iter): 2.8501436710357666, Mean Loss (over last 50 iters): 2.9426043033599854\n",
      "Iter: 650, Loss (iter): 2.8261778354644775, Mean Loss (over last 50 iters): 2.9035959243774414\n",
      "Iter: 700, Loss (iter): 3.039266347885132, Mean Loss (over last 50 iters): 2.879413366317749\n",
      "Iter: 750, Loss (iter): 2.647855758666992, Mean Loss (over last 50 iters): 2.8310763835906982\n",
      "Iter: 800, Loss (iter): 2.959866523742676, Mean Loss (over last 50 iters): 2.8089897632598877\n",
      "Iter: 850, Loss (iter): 2.719844341278076, Mean Loss (over last 50 iters): 2.835024833679199\n",
      "Iter: 900, Loss (iter): 2.677290678024292, Mean Loss (over last 50 iters): 2.8064324855804443\n",
      "Iter: 950, Loss (iter): 2.7834153175354004, Mean Loss (over last 50 iters): 2.814011335372925\n",
      "Iter: 1000, Loss (iter): 2.858006000518799, Mean Loss (over last 50 iters): 2.7877495288848877\n",
      "Iter: 1050, Loss (iter): 2.8223724365234375, Mean Loss (over last 50 iters): 2.7527105808258057\n",
      "Epoch: 4/8\n",
      "Iter: 0, Loss (iter): 2.692340612411499, Mean Loss (over last 50 iters): 2.7367353439331055\n",
      "Iter: 50, Loss (iter): 2.7764275074005127, Mean Loss (over last 50 iters): 2.747898578643799\n",
      "Iter: 100, Loss (iter): 2.524268865585327, Mean Loss (over last 50 iters): 2.685917377471924\n",
      "Iter: 150, Loss (iter): 2.679694652557373, Mean Loss (over last 50 iters): 2.69991397857666\n",
      "Iter: 200, Loss (iter): 2.5949525833129883, Mean Loss (over last 50 iters): 2.6816089153289795\n",
      "Iter: 250, Loss (iter): 2.6881706714630127, Mean Loss (over last 50 iters): 2.698084831237793\n",
      "Iter: 300, Loss (iter): 2.5699996948242188, Mean Loss (over last 50 iters): 2.6633379459381104\n",
      "Iter: 350, Loss (iter): 2.602205276489258, Mean Loss (over last 50 iters): 2.632554292678833\n",
      "Iter: 400, Loss (iter): 2.7273905277252197, Mean Loss (over last 50 iters): 2.600811719894409\n",
      "Iter: 450, Loss (iter): 2.5931830406188965, Mean Loss (over last 50 iters): 2.6103339195251465\n",
      "Iter: 500, Loss (iter): 2.5861880779266357, Mean Loss (over last 50 iters): 2.5899713039398193\n",
      "Iter: 550, Loss (iter): 2.491107940673828, Mean Loss (over last 50 iters): 2.5687012672424316\n",
      "Iter: 600, Loss (iter): 2.473443031311035, Mean Loss (over last 50 iters): 2.58866810798645\n",
      "Iter: 650, Loss (iter): 2.448403835296631, Mean Loss (over last 50 iters): 2.5466794967651367\n",
      "Iter: 700, Loss (iter): 2.7073299884796143, Mean Loss (over last 50 iters): 2.5248148441314697\n",
      "Iter: 750, Loss (iter): 2.313476324081421, Mean Loss (over last 50 iters): 2.473536729812622\n",
      "Iter: 800, Loss (iter): 2.5840718746185303, Mean Loss (over last 50 iters): 2.455726385116577\n",
      "Iter: 850, Loss (iter): 2.3579163551330566, Mean Loss (over last 50 iters): 2.4836816787719727\n",
      "Iter: 900, Loss (iter): 2.366607189178467, Mean Loss (over last 50 iters): 2.4513018131256104\n",
      "Iter: 950, Loss (iter): 2.3932440280914307, Mean Loss (over last 50 iters): 2.4430158138275146\n",
      "Iter: 1000, Loss (iter): 2.512256622314453, Mean Loss (over last 50 iters): 2.4408340454101562\n",
      "Iter: 1050, Loss (iter): 2.4975216388702393, Mean Loss (over last 50 iters): 2.4197282791137695\n",
      "Epoch: 5/8\n",
      "Iter: 0, Loss (iter): 2.338935375213623, Mean Loss (over last 50 iters): 2.4071645736694336\n",
      "Iter: 50, Loss (iter): 2.4363322257995605, Mean Loss (over last 50 iters): 2.412997245788574\n",
      "Iter: 100, Loss (iter): 2.3428328037261963, Mean Loss (over last 50 iters): 2.376070261001587\n",
      "Iter: 150, Loss (iter): 2.3758842945098877, Mean Loss (over last 50 iters): 2.4304113388061523\n",
      "Iter: 200, Loss (iter): 2.3210642337799072, Mean Loss (over last 50 iters): 2.380124807357788\n",
      "Iter: 250, Loss (iter): 2.3523569107055664, Mean Loss (over last 50 iters): 2.384840965270996\n",
      "Iter: 300, Loss (iter): 2.29093599319458, Mean Loss (over last 50 iters): 2.353299140930176\n",
      "Iter: 350, Loss (iter): 2.299126148223877, Mean Loss (over last 50 iters): 2.330674409866333\n",
      "Iter: 400, Loss (iter): 2.463127851486206, Mean Loss (over last 50 iters): 2.3030824661254883\n",
      "Iter: 450, Loss (iter): 2.3057422637939453, Mean Loss (over last 50 iters): 2.3118069171905518\n",
      "Iter: 500, Loss (iter): 2.321903705596924, Mean Loss (over last 50 iters): 2.2938308715820312\n",
      "Iter: 550, Loss (iter): 2.172464370727539, Mean Loss (over last 50 iters): 2.2803330421447754\n",
      "Iter: 600, Loss (iter): 2.1697866916656494, Mean Loss (over last 50 iters): 2.295607805252075\n",
      "Iter: 650, Loss (iter): 2.1931891441345215, Mean Loss (over last 50 iters): 2.255260705947876\n",
      "Iter: 700, Loss (iter): 2.387166738510132, Mean Loss (over last 50 iters): 2.2371606826782227\n",
      "Iter: 750, Loss (iter): 2.013558864593506, Mean Loss (over last 50 iters): 2.19063663482666\n",
      "Iter: 800, Loss (iter): 2.292877674102783, Mean Loss (over last 50 iters): 2.177640914916992\n",
      "Iter: 850, Loss (iter): 2.1050524711608887, Mean Loss (over last 50 iters): 2.209918737411499\n",
      "Iter: 900, Loss (iter): 2.0972256660461426, Mean Loss (over last 50 iters): 2.179678440093994\n",
      "Iter: 950, Loss (iter): 2.12331223487854, Mean Loss (over last 50 iters): 2.183288812637329\n",
      "Iter: 1000, Loss (iter): 2.263780355453491, Mean Loss (over last 50 iters): 2.1884605884552\n",
      "Iter: 1050, Loss (iter): 2.2582411766052246, Mean Loss (over last 50 iters): 2.166828155517578\n",
      "Epoch: 6/8\n",
      "Iter: 0, Loss (iter): 2.0677313804626465, Mean Loss (over last 50 iters): 2.153627395629883\n",
      "Iter: 50, Loss (iter): 2.174344062805176, Mean Loss (over last 50 iters): 2.147449254989624\n",
      "Iter: 100, Loss (iter): 1.9555795192718506, Mean Loss (over last 50 iters): 2.0889170169830322\n",
      "Iter: 150, Loss (iter): 2.145958185195923, Mean Loss (over last 50 iters): 2.130086898803711\n",
      "Iter: 200, Loss (iter): 2.0023715496063232, Mean Loss (over last 50 iters): 2.104612350463867\n",
      "Iter: 250, Loss (iter): 2.077355146408081, Mean Loss (over last 50 iters): 2.1231679916381836\n",
      "Iter: 300, Loss (iter): 2.03349232673645, Mean Loss (over last 50 iters): 2.0996382236480713\n",
      "Iter: 350, Loss (iter): 2.0376346111297607, Mean Loss (over last 50 iters): 2.079562187194824\n",
      "Iter: 400, Loss (iter): 2.186602830886841, Mean Loss (over last 50 iters): 2.0500216484069824\n",
      "Iter: 450, Loss (iter): 2.0363709926605225, Mean Loss (over last 50 iters): 2.052400588989258\n",
      "Iter: 500, Loss (iter): 2.1175811290740967, Mean Loss (over last 50 iters): 2.0482161045074463\n",
      "Iter: 550, Loss (iter): 1.9241434335708618, Mean Loss (over last 50 iters): 2.0413804054260254\n",
      "Iter: 600, Loss (iter): 1.9384024143218994, Mean Loss (over last 50 iters): 2.054922103881836\n",
      "Iter: 650, Loss (iter): 1.9696177244186401, Mean Loss (over last 50 iters): 2.016134738922119\n",
      "Iter: 700, Loss (iter): 2.141833543777466, Mean Loss (over last 50 iters): 2.003249168395996\n",
      "Iter: 750, Loss (iter): 1.7678637504577637, Mean Loss (over last 50 iters): 1.958681344985962\n",
      "Iter: 800, Loss (iter): 2.028413772583008, Mean Loss (over last 50 iters): 1.9445098638534546\n",
      "Iter: 850, Loss (iter): 1.8798938989639282, Mean Loss (over last 50 iters): 1.9781054258346558\n",
      "Iter: 900, Loss (iter): 1.8882845640182495, Mean Loss (over last 50 iters): 1.9586234092712402\n",
      "Iter: 950, Loss (iter): 1.8772492408752441, Mean Loss (over last 50 iters): 1.9628039598464966\n",
      "Iter: 1000, Loss (iter): 2.0680811405181885, Mean Loss (over last 50 iters): 1.9733713865280151\n",
      "Iter: 1050, Loss (iter): 2.02398681640625, Mean Loss (over last 50 iters): 1.9510853290557861\n",
      "Epoch: 7/8\n",
      "Iter: 0, Loss (iter): 1.8563300371170044, Mean Loss (over last 50 iters): 1.936997652053833\n",
      "Iter: 50, Loss (iter): 1.941358208656311, Mean Loss (over last 50 iters): 1.93035888671875\n",
      "Iter: 100, Loss (iter): 1.7578537464141846, Mean Loss (over last 50 iters): 1.8681679964065552\n",
      "Iter: 150, Loss (iter): 1.9412661790847778, Mean Loss (over last 50 iters): 1.9088674783706665\n",
      "Iter: 200, Loss (iter): 1.7697389125823975, Mean Loss (over last 50 iters): 1.890997290611267\n",
      "Iter: 250, Loss (iter): 1.828757882118225, Mean Loss (over last 50 iters): 1.901139736175537\n",
      "Iter: 300, Loss (iter): 1.8324470520019531, Mean Loss (over last 50 iters): 1.8833926916122437\n",
      "Iter: 350, Loss (iter): 1.824042797088623, Mean Loss (over last 50 iters): 1.859665870666504\n",
      "Iter: 400, Loss (iter): 1.9650959968566895, Mean Loss (over last 50 iters): 1.8431222438812256\n",
      "Iter: 450, Loss (iter): 1.817600965499878, Mean Loss (over last 50 iters): 1.839894413948059\n",
      "Iter: 500, Loss (iter): 1.8925795555114746, Mean Loss (over last 50 iters): 1.8359359502792358\n",
      "Iter: 550, Loss (iter): 1.7112493515014648, Mean Loss (over last 50 iters): 1.8345532417297363\n",
      "Iter: 600, Loss (iter): 1.74800443649292, Mean Loss (over last 50 iters): 1.843342900276184\n",
      "Iter: 650, Loss (iter): 1.782918930053711, Mean Loss (over last 50 iters): 1.8149974346160889\n",
      "Iter: 700, Loss (iter): 1.89932119846344, Mean Loss (over last 50 iters): 1.7983859777450562\n",
      "Iter: 750, Loss (iter): 1.6037325859069824, Mean Loss (over last 50 iters): 1.7583487033843994\n",
      "Iter: 800, Loss (iter): 1.8064781427383423, Mean Loss (over last 50 iters): 1.7442675828933716\n",
      "Iter: 850, Loss (iter): 1.7007073163986206, Mean Loss (over last 50 iters): 1.781337857246399\n",
      "Iter: 900, Loss (iter): 1.7213764190673828, Mean Loss (over last 50 iters): 1.7612314224243164\n",
      "Iter: 950, Loss (iter): 1.659845232963562, Mean Loss (over last 50 iters): 1.7661771774291992\n",
      "Iter: 1000, Loss (iter): 1.8869010210037231, Mean Loss (over last 50 iters): 1.7865512371063232\n",
      "Iter: 1050, Loss (iter): 1.8314458131790161, Mean Loss (over last 50 iters): 1.7564594745635986\n",
      "Epoch: 8/8\n",
      "Iter: 0, Loss (iter): 1.6659468412399292, Mean Loss (over last 50 iters): 1.7458304166793823\n",
      "Iter: 50, Loss (iter): 1.7318315505981445, Mean Loss (over last 50 iters): 1.7404109239578247\n",
      "Iter: 100, Loss (iter): 1.5751054286956787, Mean Loss (over last 50 iters): 1.6666070222854614\n",
      "Iter: 150, Loss (iter): 1.6808948516845703, Mean Loss (over last 50 iters): 1.7233343124389648\n",
      "Iter: 200, Loss (iter): 1.5631136894226074, Mean Loss (over last 50 iters): 1.705519437789917\n",
      "Iter: 250, Loss (iter): 1.6548371315002441, Mean Loss (over last 50 iters): 1.7085928916931152\n",
      "Iter: 300, Loss (iter): 1.6390557289123535, Mean Loss (over last 50 iters): 1.6925930976867676\n",
      "Iter: 350, Loss (iter): 1.6507899761199951, Mean Loss (over last 50 iters): 1.6689727306365967\n",
      "Iter: 400, Loss (iter): 1.7825894355773926, Mean Loss (over last 50 iters): 1.658644676208496\n",
      "Iter: 450, Loss (iter): 1.6204030513763428, Mean Loss (over last 50 iters): 1.6538174152374268\n",
      "Iter: 500, Loss (iter): 1.7385441064834595, Mean Loss (over last 50 iters): 1.6540231704711914\n",
      "Iter: 550, Loss (iter): 1.5194681882858276, Mean Loss (over last 50 iters): 1.6558717489242554\n",
      "Iter: 600, Loss (iter): 1.5565193891525269, Mean Loss (over last 50 iters): 1.6696923971176147\n",
      "Iter: 650, Loss (iter): 1.628380298614502, Mean Loss (over last 50 iters): 1.6436117887496948\n",
      "Iter: 700, Loss (iter): 1.699616551399231, Mean Loss (over last 50 iters): 1.6229243278503418\n",
      "Iter: 750, Loss (iter): 1.407260775566101, Mean Loss (over last 50 iters): 1.5753912925720215\n",
      "Iter: 800, Loss (iter): 1.5917781591415405, Mean Loss (over last 50 iters): 1.5641587972640991\n",
      "Iter: 850, Loss (iter): 1.523250937461853, Mean Loss (over last 50 iters): 1.6095480918884277\n",
      "Iter: 900, Loss (iter): 1.5323752164840698, Mean Loss (over last 50 iters): 1.5899053812026978\n",
      "Iter: 950, Loss (iter): 1.4768563508987427, Mean Loss (over last 50 iters): 1.5960453748703003\n",
      "Iter: 1000, Loss (iter): 1.6779930591583252, Mean Loss (over last 50 iters): 1.6104388236999512\n",
      "Iter: 1050, Loss (iter): 1.6654915809631348, Mean Loss (over last 50 iters): 1.592586874961853\n"
     ]
    }
   ],
   "source": [
    "from utils.translation.train_funcs import train_seq2seq_model\n",
    "\n",
    "# Train the model. Use the Adam optimizer with 1e-3 learning rate.\n",
    "num_epochs = 8\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "train_seq2seq_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these cells to evaluate your model on one batch of the validation set\n",
    "val_sample = val_loader.shuffle(10000).take(1)\n",
    "val_sample = next(iter(val_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 1\n",
      "Dutch Sentence:  ik wil dat dingen veranderen\n",
      "English Sentence (Truth):  i want things to change\n",
      "English Sentence (Pred) i want to be prepared\n",
      "Sample: 2\n",
      "Dutch Sentence:  tom is nog steeds niet gewend aan het leven in de stad\n",
      "English Sentence (Truth):  tom is still not accustomed to city life\n",
      "English Sentence (Pred) tom isn t very interested in the [UNK]\n",
      "Sample: 3\n",
      "Dutch Sentence:  ik dacht dat je dat wist\n",
      "English Sentence (Truth):  i thought you knew that\n",
      "English Sentence (Pred) i thought that you said that\n",
      "Sample: 4\n",
      "Dutch Sentence:  waarom houdt iedereen van katten ?\n",
      "English Sentence (Truth):  why does everybody love cats ?\n",
      "English Sentence (Pred) why do people like this ?\n",
      "Sample: 5\n",
      "Dutch Sentence:  hij studeert [UNK]\n",
      "English Sentence (Truth):  he is studying [UNK]\n",
      "English Sentence (Pred) he is studying [UNK]\n",
      "Sample: 6\n",
      "Dutch Sentence:  ze vierde gisteren haar [UNK] verjaardag\n",
      "English Sentence (Truth):  she celebrated her fifteenth birthday yesterday\n",
      "English Sentence (Pred) she decided not to go to bed with her\n",
      "Sample: 7\n",
      "Dutch Sentence:  doe uw schoenen uit\n",
      "English Sentence (Truth):  take off your shoes\n",
      "English Sentence (Pred) put your hat on\n",
      "Sample: 8\n",
      "Dutch Sentence:  tom is de bijbel aan het lezen\n",
      "English Sentence (Truth):  tom is reading the bible\n",
      "English Sentence (Pred) tom is the star of the [UNK]\n",
      "Sample: 9\n",
      "Dutch Sentence:  kan je het licht uitdoen ?\n",
      "English Sentence (Truth):  could you turn off the lights ?\n",
      "English Sentence (Pred) can you drive a little bit from the room ?\n",
      "Sample: 10\n",
      "Dutch Sentence:  de oude man is heel vriendelijk\n",
      "English Sentence (Truth):  the old man is very kind\n",
      "English Sentence (Pred) the sun is about to the airport\n",
      "Sample: 11\n",
      "Dutch Sentence:  waar denkt u aan ?\n",
      "English Sentence (Truth):  what re you thinking about ?\n",
      "English Sentence (Pred) what do you want ?\n"
     ]
    }
   ],
   "source": [
    "val_inp, val_target = val_sample\n",
    "decoded_inputs = []\n",
    "reserved_tokens = ['[PAD]', '[SOS]', '[EOS]']\n",
    "for inp in val_inp.numpy():\n",
    "    decoded_text = decode_text(inp, vocab=nl_vocab)\n",
    "    decoded_inputs.append([token for token in decoded_text if token not in reserved_tokens])\n",
    "\n",
    "val_pred = model(val_inp, training=False)\n",
    "decoded_outputs = model.decode_tokens(val_pred)\n",
    "decoded_ground_truth = model.decode_tokens(val_target[:, 1:])\n",
    "\n",
    "samples_to_show = 10 #Should be <= batch size\n",
    "\n",
    "for i, decoded_data in enumerate(zip(decoded_inputs, decoded_ground_truth, decoded_outputs)):\n",
    "    nl_sentence = ' '.join(decoded_data[0])\n",
    "    gt_en_sentence = ' '.join(decoded_data[1])\n",
    "    pred_en_sentence = ' '.join(decoded_data[2])\n",
    "    print('Sample:', i+1)\n",
    "    print('Dutch Sentence: ', nl_sentence)\n",
    "    print('English Sentence (Truth): ', gt_en_sentence)\n",
    "    print('English Sentence (Pred)', pred_en_sentence)\n",
    "    if i > samples_to_show - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>BONUS TODO:</strong></font> <b> Briefly describe the differences in quality of the translations between the model with and without a bidirectional encoder. (1-2 sentenes is enough) <b/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Answer:__</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general both models have a sufficient level of accuracy as the difference in the loss functions is within the limits mentioned in edstem. However, the model with the encoder seems slightly more fluent and with a better understanding towards uncertainties. I thinks this should be more visible in he case of longer sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Assignment_3_task_2_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "36142657f443a869bd2c1b509e6f1df9b014ad48aa206cdd00d27f8f22cb37ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
